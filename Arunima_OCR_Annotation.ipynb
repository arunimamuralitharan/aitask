{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPnaYZx6XCSgao3VahOe/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunimamuralitharan/aitask/blob/main/Arunima_OCR_Annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMQoGUWbz_r6",
        "outputId": "47a509ec-868f-4500-ba67-2602078766ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAXFtHcLyntH",
        "outputId": "dd00b337-65fb-48eb-b0da-8843a3d2a6fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Improved CPU-Optimized Invoice OCR System\n",
            "==================================================\n",
            "Loading SROIE dataset\n",
            "Downloading SROIE dataset\n",
            "Dataset downloaded to: /kaggle/input/sroie-datasetv2\n",
            "Found 973 image files and 1947 text files\n",
            "Limiting dataset to first 100 images for performance\n",
            "Loaded 100 images with annotations\n",
            "Sample annotation: {'vendor_name': 'HENG KEE DELIGHTS BAK KUT TEH.', 'total_amount': '42.00', 'date': '04/01/2018'}\n",
            " Processing SROIE Dataset...\n",
            " Processing 35 training images...\n",
            "Processing 35 invoices...\n",
            "Processing 1/35: X51005663274.jpg\n",
            "  Extracted: {'vendor_name': 'LST RM Co', 'total_amount': '10.50', 'date': '08/02/2018'}\n",
            "Processing 2/35: X51005442322.jpg\n",
            "  Extracted: {'vendor_name': 'TS,', 'total_amount': '', 'date': ''}\n",
            "Processing 3/35: X51006349081.jpg\n",
            "  Extracted: {'vendor_name': 'tee Co', 'total_amount': '80.00', 'date': '26/04/2018'}\n",
            "Processing 4/35: X51005568894.jpg\n",
            "  Extracted: {'vendor_name': '', 'total_amount': '16.00', 'date': ''}\n",
            "Processing 5/35: X51007231343.jpg\n",
            "  Extracted: {'vendor_name': 'BarWangRice', 'total_amount': '6.60', 'date': '04 May 2018'}\n",
            "Processing 6/35: X51005715007.jpg\n",
            "  Extracted: {'vendor_name': 'DAYS ACCOMPANTED BY ORIGINAL RECEIPT,', 'total_amount': '8.90', 'date': '25-03-18'}\n",
            "Processing 7/35: X51006392167.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006392167.jpg\n",
            "Processing 8/35: X51006008082.jpg\n",
            "  Extracted: {'vendor_name': 'SR RINTING, sITcs', 'total_amount': '', 'date': ''}\n",
            "Processing 9/35: X51005444044.jpg\n",
            "  Extracted: {'vendor_name': 's lo A BATRA AAP REREAD A ADD BAA LAR AmMEL han...', 'total_amount': '5.35', 'date': '10-03-18'}\n",
            "Processing 10/35: X51006401977.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006401977.jpg\n",
            "Processing 11/35: X51005719889.jpg\n",
            "  Extracted: {'vendor_name': 'OME AGAIN .. a oa', 'total_amount': '42.00', 'date': '04/01/2018'}\n",
            "Processing 12/35: X51007225406.jpg\n",
            "  Extracted: {'vendor_name': 'you. oo Please come again. Goods sold non returna', 'total_amount': '23.49', 'date': '2 Subtotal 23'}\n",
            "Processing 13/35: X51005447851.jpg\n",
            "  Extracted: {'vendor_name': '', 'total_amount': '', 'date': ''}\n",
            "Processing 14/35: X51006619704.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006619704.jpg\n",
            "Processing 15/35: X51006619842.jpg\n",
            "  Extracted: {'vendor_name': 'EXCHANGE ARE ALLOWED WITHIN', 'total_amount': '32.70', 'date': '03 Jun 2016'}\n",
            "Processing 16/35: X51005447841.jpg\n",
            "  Extracted: {'vendor_name': 'CASHIER: 2 Exe; Amt Ine. GST” WALK “2490 2 vd 1O0PLUS REGULAR ‘500M.', 'total_amount': '', 'date': ''}\n",
            "Processing 17/35: X51006619869.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006619869.jpg\n",
            "Processing 18/35: X51006828201.jpg\n",
            "  Extracted: {'vendor_name': 'Pear ab ne THE CO', 'total_amount': '12.08', 'date': '5/12/2017'}\n",
            "Processing 19/35: X51005447859.jpg\n",
            "  Extracted: {'vendor_name': 'Nore Total Inc', 'total_amount': '4.45', 'date': ''}\n",
            "Processing 20/35: X51007231372.jpg\n",
            "  Extracted: {'vendor_name': 'Powered by Genius POS', 'total_amount': '38.35', 'date': '02 Total 38'}\n",
            "Processing 21/35: X51006619863.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006619863.jpg\n",
            "Processing 22/35: X51005806696.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51005806696.jpg\n",
            "Processing 23/35: X51007846387.jpg\n",
            "  Extracted: {'vendor_name': 'AMOUNT TAX RM RM CO', 'total_amount': '0.00', 'date': '1/6/2018'}\n",
            "Processing 24/35: X51007339161.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51007339161.jpg\n",
            "Processing 25/35: X51006328967.jpg\n",
            "  Extracted: {'vendor_name': 'SDN BHD CO', 'total_amount': '62.88', 'date': '17/03/2017'}\n",
            "Processing 26/35: X51005444046.jpg\n",
            "  Extracted: {'vendor_name': '', 'total_amount': '0.85', 'date': '25/03/18'}\n",
            "Processing 27/35: X51006619703.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006619703.jpg\n",
            "Processing 28/35: X51006556654.jpg\n",
            "  Extracted: {'vendor_name': 'E ve S COPY', 'total_amount': '17.82', 'date': '18/03/2017'}\n",
            "Processing 29/35: X51005230616.jpg\n",
            "  Extracted: {'vendor_name': 'tee', 'total_amount': '25.40', 'date': '18/91/2018'}\n",
            "Processing 30/35: X51005746210.jpg\n",
            "  Extracted: {'vendor_name': 'lan Co', 'total_amount': '0.08', 'date': '12 Har 2018'}\n",
            "Processing 31/35: X51005719857.jpg\n",
            "  Extracted: {'vendor_name': 'CW tee Re', 'total_amount': '1.35', 'date': '19-02-2018'}\n",
            "Processing 32/35: X51006647984.jpg\n",
            "  Extracted: {'vendor_name': 'Kechara Oasis Kechara Vegetarian Restaurant', 'total_amount': '55.', 'date': '1/05/18'}\n",
            "Processing 33/35: X51005444041.jpg\n",
            "  Extracted: {'vendor_name': 'PLEASE CO', 'total_amount': '81.00', 'date': '08/03/18'}\n",
            "Processing 34/35: X51006554841.jpg\n",
            "  Extracted: {'vendor_name': 'HART TERIMA CASTH Dat SILA DATARG LAGI', 'total_amount': '9.00', 'date': ''}\n",
            "Processing 35/35: X51006414512.jpg\n",
            "  Extracted: {'vendor_name': 'ce Qnty Amount Thank You For Shop Customer service', 'total_amount': '', 'date': ''}\n",
            " Processing 15 test images...\n",
            "Processing 15 invoices...\n",
            "Processing 1/15: X51007846403.jpg\n",
            "  Extracted: {'vendor_name': 'U, PLEASE COME AGAIN Goods Sold Are Non Refundable', 'total_amount': '13.10', 'date': ''}\n",
            "Processing 2/15: X51006401845.jpg\n",
            "  Extracted: {'vendor_name': 'Thank You. Pleese Come Agate.', 'total_amount': '', 'date': ''}\n",
            "Processing 3/15: X51006556810.jpg\n",
            "  Extracted: {'vendor_name': 'vetment Hot s tOME Re SCO', 'total_amount': '1.53', 'date': '20/09/2017'}\n",
            "Processing 4/15: X51006008105.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006008105.jpg\n",
            "Processing 5/15: X51007433809.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51007433809.jpg\n",
            "Processing 6/15: X51006857132.jpg\n",
            "  Extracted: {'vendor_name': '', 'total_amount': '8.60', 'date': ''}\n",
            "Processing 7/15: X51006556648.jpg\n",
            "  Extracted: {'vendor_name': 'cu STON', 'total_amount': '11.88', 'date': '20/08/2017'}\n",
            "Processing 8/15: X51005806692.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51005806692.jpg\n",
            "Processing 9/15: X51005763958.jpg\n",
            "  Extracted: {'vendor_name': 'THANK YOU a', 'total_amount': '5.90', 'date': '27-01-2018'}\n",
            "Processing 10/15: X51006328345.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006328345.jpg\n",
            "Processing 11/15: X51005675104.jpg\n",
            "  Extracted: {'vendor_name': 'Disco', 'total_amount': '180.00', 'date': '25/01/2018'}\n",
            "Processing 12/15: X51006733495.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51006733495.jpg\n",
            "Processing 13/15: X51006557115.jpg\n",
            "  Extracted: {'vendor_name': 'on the basic. of the show invents Tax Thvoice a', 'total_amount': '', 'date': '2/10/2617'}\n",
            "Processing 14/15: X51005719863.jpg\n",
            "No text extracted from /kaggle/input/sroie-datasetv2/SROIE2019/test/img/X51005719863.jpg\n",
            "Processing 15/15: X51006619784.jpg\n",
            "  Extracted: {'vendor_name': 'arene ranean Fetal Arad inc', 'total_amount': '4.00.00', 'date': ''}\n",
            "\n",
            " Evaluating System Performance...\n",
            "========================================\n",
            "  vendor_name: 'u, please come again goods sold are non refundable' vs 'royaltea'\n",
            "  vendor_name: 'thank you. pleese come agate.' vs 'buy sell trade sdn. bhd.'\n",
            "  vendor_name: 'vetment hot s tome re sco' vs 'gardenia bakeries (kl) sdn bhd'\n",
            "  vendor_name: 'cu ston' vs 'gardenia bakeries (kl) sdn bhd'\n",
            "  vendor_name: 'thank you a' vs 'super seven cash & carry sdn bhd'\n",
            "  vendor_name: 'disco' vs 'syarikat perniagaan gin kee'\n",
            "  vendor_name: 'on the basic. of the show invents tax thvoice a' vs 'gardenia bakeries (kl) sdn bhd'\n",
            "  vendor_name: 'arene ranean fetal arad inc' vs 'security & oa trading'\n",
            "Vendor Name: 0.00% (0/15)\n",
            "  total_amount: '13.10' vs '13.10'\n",
            "  total_amount: '1.53' vs '23.02'\n",
            "  total_amount: '8.60' vs '23.80'\n",
            "  total_amount: '11.88' vs '55.14'\n",
            "  total_amount: '5.90' vs 'rm59.00'\n",
            "  total_amount: '180.00' vs '190.80'\n",
            "  total_amount: '4.00.00' vs '399.00'\n",
            "Total Amount: 6.67% (1/15)\n",
            "  date: '20/09/2017' vs '20/09/2017'\n",
            "  date: '20/08/2017' vs '20/08/2017'\n",
            "  date: '27-01-2018' vs '27-01-2018'\n",
            "  date: '25/01/2018' vs '25/01/2018'\n",
            "  date: '2/10/2617' vs '02/10/2017'\n",
            "Date: 26.67% (4/15)\n",
            "\n",
            "Overall Accuracy: 11.11%\n",
            "Results saved to invoice_extraction_results.json\n",
            "\n",
            "System Performance Summary:\n",
            "Overall Accuracy: 11.11%\n",
            "  System needs improvement\n",
            " Processing complete!\n",
            "\n",
            "  Total Runtime: 2682.99 seconds\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class InvoiceOCRSystem:\n",
        "\n",
        "    def __init__(self, dataset_path=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.model_pipeline = None\n",
        "        self.field_extractors = {}\n",
        "        self.annotations = {}\n",
        "        self.images_path = None\n",
        "        self.annotations_path = None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        print(\"Loading SROIE dataset\")\n",
        "\n",
        "        if self.dataset_path is None:\n",
        "            try:\n",
        "                import kagglehub\n",
        "                print(\"Downloading SROIE dataset\")\n",
        "                self.dataset_path = kagglehub.dataset_download(\"urbikn/sroie-datasetv2\")\n",
        "                print(f\"Dataset downloaded to: {self.dataset_path}\")\n",
        "            except ImportError:\n",
        "                print(\"Kagglehub not installed.\")\n",
        "                return [], []\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading dataset: {str(e)}\")\n",
        "                return [], []\n",
        "\n",
        "        #Find images and annotations\n",
        "        images = []\n",
        "        annotations = []\n",
        "\n",
        "        #Looking files in the dataset directory\n",
        "        all_files = []\n",
        "        for root, dirs, files in os.walk(self.dataset_path):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                all_files.append(full_path)\n",
        "\n",
        "        #Separate images and text files\n",
        "        image_files = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        text_files = [f for f in all_files if f.lower().endswith('.txt')]\n",
        "\n",
        "        print(f\"Found {len(image_files)} image files and {len(text_files)} text files\")\n",
        "\n",
        "        #Match images with entity annotation files\n",
        "        for img_path in image_files:\n",
        "            img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "            #Look for entity annotation file first\n",
        "            entity_file = None\n",
        "            for txt_path in text_files:\n",
        "                if 'entities' in txt_path and img_name in txt_path:\n",
        "                    entity_file = txt_path\n",
        "                    break\n",
        "\n",
        "            if entity_file:\n",
        "                try:\n",
        "                    with open(entity_file, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read().strip()\n",
        "                        ann_data = self.parse_entity_annotation(content)\n",
        "                        if any(ann_data.values()):\n",
        "                            images.append(img_path)\n",
        "                            annotations.append(ann_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {entity_file}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        #Limiting dataset size for performance\n",
        "        max_images = 100\n",
        "        if len(images) > max_images:\n",
        "            print(f\"Limiting dataset to first {max_images} images for performance\")\n",
        "            images = images[:max_images]\n",
        "            annotations = annotations[:max_images]\n",
        "\n",
        "        print(f\"Loaded {len(images)} images with annotations\")\n",
        "        if annotations:\n",
        "            print(f\"Sample annotation: {annotations[0]}\")\n",
        "\n",
        "        return images, annotations\n",
        "\n",
        "    def parse_entity_annotation(self, content):\n",
        "        annotation = {'vendor_name': '', 'total_amount': '', 'date': ''}\n",
        "\n",
        "        try:\n",
        "            #Parse JSON-like format\n",
        "            data = json.loads(content)\n",
        "\n",
        "            #Map SROIE fields to fields\n",
        "            if 'company' in data:\n",
        "                annotation['vendor_name'] = str(data['company']).strip('\"')\n",
        "            elif 'vendor' in data:\n",
        "                annotation['vendor_name'] = str(data['vendor']).strip('\"')\n",
        "\n",
        "            if 'total' in data:\n",
        "                annotation['total_amount'] = str(data['total']).strip('\"$')\n",
        "            elif 'amount' in data:\n",
        "                annotation['total_amount'] = str(data['amount']).strip('\"$')\n",
        "\n",
        "            if 'date' in data:\n",
        "                annotation['date'] = str(data['date']).strip('\"')\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            lines = content.strip().split('\\n')\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                if ':' in line:\n",
        "                    key, value = line.split(':', 1)\n",
        "                    key = key.strip().lower()\n",
        "                    value = value.strip().strip('\"')\n",
        "\n",
        "                    if 'company' in key or 'vendor' in key:\n",
        "                        annotation['vendor_name'] = value\n",
        "                    elif 'total' in key or 'amount' in key:\n",
        "                        annotation['total_amount'] = value.strip('$')\n",
        "                    elif 'date' in key:\n",
        "                        annotation['date'] = value\n",
        "\n",
        "        return annotation\n",
        "\n",
        "    def advanced_preprocessing(self, image_path):\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            return None\n",
        "\n",
        "        #To grayscale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        #1.Noise reduction\n",
        "        denoised = cv2.fastNlMeansDenoising(gray)\n",
        "\n",
        "        #2.Contrast enhancement using CLAHE\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        enhanced = clahe.apply(denoised)\n",
        "\n",
        "        #3.Gaussian blur to smooth\n",
        "        blurred = cv2.GaussianBlur(enhanced, (1, 1), 0)\n",
        "\n",
        "        #4.Adaptive thresholding\n",
        "        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                     cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "        #5.Morphological operations\n",
        "        kernel = np.ones((1, 1), np.uint8)\n",
        "        cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        #6.Dilation to make text thicker\n",
        "        kernel2 = np.ones((1, 1), np.uint8)\n",
        "        final = cv2.dilate(cleaned, kernel2, iterations=1)\n",
        "\n",
        "        return final\n",
        "\n",
        "    def extract_text_with_ocr(self, image_path):\n",
        "        \"\"\"Extract text from image using improved OCR\"\"\"\n",
        "        try:\n",
        "            processed_imgs = []\n",
        "\n",
        "            processed_imgs.append(self.preprocess_image(image_path))\n",
        "\n",
        "            #Advanced preprocessing\n",
        "            advanced_processed = self.advanced_preprocessing(image_path)\n",
        "            if advanced_processed is not None:\n",
        "                processed_imgs.append(advanced_processed)\n",
        "\n",
        "            best_text = \"\"\n",
        "            best_confidence = 0\n",
        "\n",
        "            #Different OCR config\n",
        "            configs = [\n",
        "                '--oem 3 --psm 6 -l eng',\n",
        "                '--oem 3 --psm 4 -l eng',\n",
        "                '--oem 3 --psm 3 -l eng',\n",
        "                '--oem 1 --psm 6 -l eng'\n",
        "            ]\n",
        "\n",
        "            for img in processed_imgs:\n",
        "                for config in configs:\n",
        "                    try:\n",
        "                        #Get text with confidence\n",
        "                        data = pytesseract.image_to_data(img, config=config, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "                        #Calculate average confidence\n",
        "                        confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]\n",
        "                        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
        "\n",
        "                        if avg_confidence > best_confidence:\n",
        "                            best_confidence = avg_confidence\n",
        "                            best_text = pytesseract.image_to_string(img, config=config)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "            return best_text if best_text else \"\", {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {str(e)}\")\n",
        "            return \"\", {}\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            return None\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                     cv2.THRESH_BINARY, 11, 2)\n",
        "        kernel = np.ones((2, 2), np.uint8)\n",
        "        cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "        return cleaned\n",
        "\n",
        "    def extract_fields_with_improved_patterns(self, text):\n",
        "        fields = {\n",
        "            'vendor_name': '',\n",
        "            'total_amount': '',\n",
        "            'date': ''\n",
        "        }\n",
        "\n",
        "        #Clean and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text)  #Normalize whitespace\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        #Vendor name patterns\n",
        "        vendor_patterns = [\n",
        "            r'([A-Z][A-Z\\s&.,\\-]{2,50}?)(?=\\n|$)',\n",
        "            r'^([A-Z][A-Za-z\\s&.,\\-]{3,50}?)(?=\\n)',\n",
        "            r'((?:[A-Z][a-z]+\\s*){1,4}(?:Corp|Inc|Ltd|LLC|Company|Co|Store|Mart|Shop|Restaurant|Cafe|Hotel))',\n",
        "            r'^(.+?)(?=\\n.*(?:invoice|receipt|bill))',\n",
        "        ]\n",
        "\n",
        "        #Total amount patterns\n",
        "        total_patterns = [\n",
        "            r'(?:total|amount|sum|grand\\s*total|balance|due)[:\\s]*\\$?\\s*(\\d+[,.]?\\d*\\.?\\d*)',\n",
        "            r'(?:total|amount|sum)[:\\s]*(\\d+[,.]?\\d*\\.?\\d*)',\n",
        "            r'\\$\\s*(\\d+[,.]?\\d*\\.?\\d*)\\s*(?:total|$|\\n)',\n",
        "            r'(\\d+\\.\\d{2})\\s*(?:total|amount|sum|$)',\n",
        "            r'(?:rm|usd|\\$)\\s*(\\d+[,.]?\\d*\\.?\\d*)',\n",
        "            r'(\\d+[,.]?\\d{3}\\.\\d{2})',\n",
        "            r'(\\d+\\.\\d{2})',\n",
        "        ]\n",
        "\n",
        "        #Date patterns\n",
        "        date_patterns = [\n",
        "            r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',\n",
        "            r'(\\d{1,2}\\s+[A-Za-z]{3,9}\\s+\\d{2,4})',\n",
        "            r'([A-Za-z]{3,9}\\s+\\d{1,2},?\\s+\\d{2,4})',\n",
        "            r'(\\d{2,4}-\\d{1,2}-\\d{1,2})',\n",
        "            r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',\n",
        "            r'date[:\\s]*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',\n",
        "            r'(\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{2,4})',\n",
        "        ]\n",
        "\n",
        "        #Extract vendor name\n",
        "        for pattern in vendor_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
        "            if match:\n",
        "                candidate = match.group(1).strip()\n",
        "                if len(candidate) > 2 and not re.match(r'^\\d+$', candidate):\n",
        "                    fields['vendor_name'] = candidate\n",
        "                    break\n",
        "\n",
        "        #If no vendor found, try first meaningful line\n",
        "        if not fields['vendor_name']:\n",
        "            for line in lines[:3]:  #Check first 3\n",
        "                line = line.strip()\n",
        "                if len(line) > 2 and not re.match(r'^\\d', line) and line[0].isupper():\n",
        "                    fields['vendor_name'] = line\n",
        "                    break\n",
        "\n",
        "        #Extract total amount\n",
        "        for pattern in total_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                amount = match.group(1).strip()\n",
        "                #Clean amount\n",
        "                amount = re.sub(r'[^\\d.]', '', amount)\n",
        "                if amount and '.' in amount:\n",
        "                    fields['total_amount'] = amount\n",
        "                    break\n",
        "\n",
        "        #Extract date\n",
        "        for pattern in date_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                date_str = match.group(1).strip()\n",
        "                if date_str:\n",
        "                    fields['date'] = date_str\n",
        "                    break\n",
        "\n",
        "        return fields\n",
        "\n",
        "    def process_invoices(self, image_paths, enable_annotation=False):\n",
        "        results = []\n",
        "\n",
        "        print(f\"Processing {len(image_paths)} invoices...\")\n",
        "\n",
        "        for i, image_path in enumerate(image_paths):\n",
        "            print(f\"Processing {i+1}/{len(image_paths)}: {os.path.basename(image_path)}\")\n",
        "\n",
        "            #Extract text\n",
        "            text, ocr_data = self.extract_text_with_ocr(image_path)\n",
        "\n",
        "            if not text.strip():\n",
        "                print(f\"No text extracted from {image_path}\")\n",
        "                results.append({\n",
        "                    'image_path': image_path,\n",
        "                    'text': '',\n",
        "                    'fields': {'vendor_name': '', 'total_amount': '', 'date': ''}\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            #Extract improved patterns\n",
        "            fields = self.extract_fields_with_improved_patterns(text)\n",
        "\n",
        "            #Debug output\n",
        "            print(f\"  Extracted: {fields}\")\n",
        "\n",
        "            results.append({\n",
        "                'image_path': image_path,\n",
        "                'text': text,\n",
        "                'fields': fields\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_system(self, test_data, ground_truth):\n",
        "        print(\"\\n Evaluating System Performance...\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        field_accuracies = {}\n",
        "\n",
        "        for field_name in ['vendor_name', 'total_amount', 'date']:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for result, truth in zip(test_data, ground_truth):\n",
        "                if field_name in truth and truth[field_name]:\n",
        "                    total += 1\n",
        "                    extracted = result['fields'][field_name].lower().strip()\n",
        "                    actual = truth[field_name].lower().strip()\n",
        "\n",
        "                    if not extracted or not actual:\n",
        "                        continue\n",
        "\n",
        "                    print(f\"  {field_name}: '{extracted}' vs '{actual}'\")\n",
        "\n",
        "                    if field_name == 'vendor_name':\n",
        "                        if extracted == actual:\n",
        "                            correct += 1\n",
        "                        elif extracted in actual or actual in extracted:\n",
        "                            correct += 1\n",
        "                        else:\n",
        "                            extracted_words = set(extracted.split())\n",
        "                            actual_words = set(actual.split())\n",
        "                            if extracted_words & actual_words:\n",
        "                                correct += 1\n",
        "                            else:\n",
        "                                #Fuzzy matching for similar names\n",
        "                                if self.fuzzy_match(extracted, actual, threshold=0.7):\n",
        "                                    correct += 1\n",
        "\n",
        "                    elif field_name == 'total_amount':\n",
        "                        #Clean both amounts\n",
        "                        extracted_clean = re.sub(r'[^\\d.]', '', extracted)\n",
        "                        actual_clean = re.sub(r'[^\\d.]', '', actual)\n",
        "\n",
        "                        if extracted_clean == actual_clean:\n",
        "                            correct += 1\n",
        "                        else:\n",
        "                            try:\n",
        "                                if float(extracted_clean) == float(actual_clean):\n",
        "                                    correct += 1\n",
        "                            except ValueError:\n",
        "                                pass\n",
        "\n",
        "                    elif field_name == 'date':\n",
        "                        if extracted == actual:\n",
        "                            correct += 1\n",
        "                        else:\n",
        "                            extracted_norm = self.normalize_date(extracted)\n",
        "                            actual_norm = self.normalize_date(actual)\n",
        "                            if extracted_norm == actual_norm:\n",
        "                                correct += 1\n",
        "\n",
        "            accuracy = correct / total if total > 0 else 0\n",
        "            field_accuracies[field_name] = accuracy\n",
        "            print(f\"{field_name.replace('_', ' ').title()}: {accuracy:.2%} ({correct}/{total})\")\n",
        "\n",
        "        overall_accuracy = sum(field_accuracies.values()) / len(field_accuracies) if field_accuracies else 0\n",
        "        print(f\"\\nOverall Accuracy: {overall_accuracy:.2%}\")\n",
        "\n",
        "        return field_accuracies, overall_accuracy\n",
        "\n",
        "    def fuzzy_match(self, str1, str2, threshold=0.7):\n",
        "       #Simple fuzzy string matching\n",
        "        #Simple character-based similarity\n",
        "        if not str1 or not str2:\n",
        "            return False\n",
        "\n",
        "        longer = str1 if len(str1) > len(str2) else str2\n",
        "        shorter = str2 if len(str1) > len(str2) else str1\n",
        "\n",
        "        matches = sum(1 for a, b in zip(longer, shorter) if a == b)\n",
        "        similarity = matches / len(longer)\n",
        "\n",
        "        return similarity >= threshold\n",
        "\n",
        "    def normalize_date(self, date_str):\n",
        "        if not date_str:\n",
        "            return \"\"\n",
        "\n",
        "        #Extract just numbers and separators\n",
        "        normalized = re.sub(r'[^\\d/\\-]', '', date_str)\n",
        "        return normalized\n",
        "\n",
        "    def save_results(self, results, output_file=\"invoice_extraction_results.json\"):\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "\n",
        "    def train_field_classifiers(self, training_data):\n",
        "        print(\"Training field classifiers...\")\n",
        "\n",
        "        print(\" Field classifiers trained\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\" Improved CPU-Optimized Invoice OCR System\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    #Initialize system\n",
        "    ocr_system = InvoiceOCRSystem()\n",
        "\n",
        "    #Load SROIE dataset\n",
        "    images, annotations = ocr_system.load_dataset()\n",
        "\n",
        "    if images and annotations:\n",
        "        print(\" Processing SROIE Dataset...\")\n",
        "\n",
        "        #Limit data for demo\n",
        "        max_samples = min(50, len(images))\n",
        "        images = images[:max_samples]\n",
        "        annotations = annotations[:max_samples]\n",
        "\n",
        "        # Split data\n",
        "        train_images, test_images = train_test_split(images, test_size=0.3, random_state=42)\n",
        "        train_annotations, test_annotations = train_test_split(annotations, test_size=0.3, random_state=42)\n",
        "\n",
        "        #Process training data\n",
        "        print(f\" Processing {len(train_images)} training images...\")\n",
        "        train_results = ocr_system.process_invoices(train_images)\n",
        "\n",
        "        #Process test data\n",
        "        print(f\" Processing {len(test_images)} test images...\")\n",
        "        test_results = ocr_system.process_invoices(test_images)\n",
        "\n",
        "        #Evaluate system\n",
        "        field_accuracies, overall_accuracy = ocr_system.evaluate_system(test_results, test_annotations)\n",
        "\n",
        "        ocr_system.save_results(test_results)\n",
        "\n",
        "    else:\n",
        "        print(\"No dataset loaded - please check dataset path\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nSystem Performance Summary:\")\n",
        "    print(f\"Overall Accuracy: {overall_accuracy:.2%}\")\n",
        "\n",
        "    if overall_accuracy >= 0.3:\n",
        "        print(\" System shows promising results\")\n",
        "    else:\n",
        "        print(\"  System needs improvement\")\n",
        "\n",
        "    print(\" Processing complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n  Process interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n  Total Runtime: {end_time - start_time:.2f} seconds\")"
      ]
    }
  ]
}